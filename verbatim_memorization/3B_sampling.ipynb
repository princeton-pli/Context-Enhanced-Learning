{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_IM_path = # Path to CEL trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_IM_path)\n",
    "ds_sample = json.load(open(model_IM_path + \"/ds_sample.json\"))\n",
    "model = AutoModelForCausalLM.from_pretrained(model_IM_path, device_map='auto')\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'α', 'β', 'Γ', 'γ', 'Δ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'Λ', 'λ', 'μ', 'ν', 'Ξ', 'ξ', 'Π', 'π', 'Σ', 'σ', 'τ', 'υ', 'Φ', 'φ', 'χ', 'Ψ', 'ψ', 'Ω', 'ω', 'क', 'ख', 'ग', 'घ', 'च', 'छ', 'ज', 'झ', 'ट', 'ठ', 'ड', 'ढ', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'स', 'ह', 'б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н', 'п', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'и', 'э', 'ю', 'я']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
      "[362, 426, 356, 423, 469, 435, 480, 473, 358, 622]\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T']\n",
      "[735, 445, 386, 452, 507, 393, 1229, 432, 328, 350]\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd']\n",
      "[549, 650, 468, 1630, 816, 1901, 264, 293, 272, 294]\n",
      "['e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n']\n",
      "[384, 282, 342, 305, 602, 503, 597, 326, 296, 308]\n",
      "['o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x']\n",
      "[297, 281, 2874, 436, 274, 259, 577, 348, 289, 865]\n",
      "['y', 'z', 'α', 'β', 'Γ', 'γ', 'Δ', 'δ', 'ε', 'ζ']\n",
      "[379, 1167, 19581, 34318, 85316, 63127, 82263, 70434, 60247, 105092]\n"
     ]
    }
   ],
   "source": [
    "letter_inds = []\n",
    "alphabet_size = 10\n",
    "for level in range(6):\n",
    "    letter_inds.append(letters[level*alphabet_size:(level+1)*alphabet_size])\n",
    "\n",
    "letter_token_inds = []\n",
    "for level in range(6):\n",
    "    letter_token_inds.append([tokenizer.encode(' ' + letter)[1] for letter in letter_inds[level]])\n",
    "\n",
    "for level in range(6):\n",
    "    print(letter_inds[level])\n",
    "    print(letter_token_inds[level])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line_token = \":\\n\"\n",
    "sep_token = \" ->\"\n",
    "\n",
    "def get_output_inds(input_ids, tokenizer):\n",
    "    token_ls = [tokenizer.decode(s) for s in input_ids[0]]\n",
    "\n",
    "    sep_inds = [[]]\n",
    "    for i, s in enumerate(token_ls):\n",
    "        if s == new_line_token:\n",
    "            if len(sep_inds[-1]) > 0:\n",
    "                sep_inds.append([])\n",
    "        if s == sep_token:\n",
    "            sep_inds[-1].append(i)\n",
    "            sep_inds[-1].append(i+1)\n",
    "\n",
    "    return sep_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = []\n",
    "for k in [\"IM_test_DMaskoff\"]:\n",
    "    for s in ds_sample[k]:\n",
    "        input_seqs.append(s['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1495107/1355119543.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores_on_inds_strong[:, torch.tensor(key_inds)] += 100\n",
      "/tmp/ipykernel_1495107/1355119543.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores_on_inds_strong[:, torch.tensor(key_inds)] += 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None:\t $0.26\\%$ & $2.05\\%$ & $0.05\\%$ & $1.13\\%$\n",
      "Weak:\t $0.33\\%$ & $2.05\\%$ & $0.07\\%$ & $1.13\\%$\n",
      "Strong:\t $2.11\\%$ & $2.05\\%$ & $1.91\\%$ & $1.34\\%$\n"
     ]
    }
   ],
   "source": [
    "temperature = 1\n",
    "\n",
    "n_levels = 5\n",
    "\n",
    "tuple_probs_raw = [[] for _ in range(n_levels)]\n",
    "tuple_probs_weak = [[] for _ in range(n_levels)]\n",
    "tuple_probs_strong = [[] for _ in range(n_levels)]\n",
    "\n",
    "for seq in input_seqs:\n",
    "    input_ids = tokenizer.encode(seq, return_tensors='pt').to(model.device)\n",
    "    sep_inds = get_output_inds(input_ids[:, :-1], tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "    gt_outputs = input_ids[:, 1:]\n",
    "\n",
    "    for level, inds in enumerate(sep_inds):\n",
    "\n",
    "        gt_on_inds = gt_outputs[0][torch.tensor(inds).to(model.device)]\n",
    "\n",
    "        scores_on_inds = outputs.logits[0][torch.tensor(inds).to(model.device)]\n",
    "        prob_on_inds_raw = F.softmax(scores_on_inds / temperature, dim=-1)\n",
    "        gt_prob_raw = prob_on_inds_raw[torch.arange(len(prob_on_inds_raw)), gt_on_inds]\n",
    "        tuple_prob_raw = gt_prob_raw[::2] * gt_prob_raw[1::2]\n",
    "\n",
    "        scores_on_inds_weak = scores_on_inds.clone().detach()\n",
    "        scores_on_inds_weak[:, 128108] = -10000\n",
    "        prob_on_inds_weak = F.softmax(scores_on_inds_weak / temperature, dim=-1)\n",
    "        gt_prob_weak = prob_on_inds_weak[torch.arange(len(prob_on_inds_weak)), gt_on_inds]\n",
    "        tuple_prob_weak = gt_prob_weak[::2] * gt_prob_weak[1::2]\n",
    "\n",
    "        key_inds = torch.tensor(letter_token_inds[level + 1]).to(model.device)\n",
    "        scores_on_inds_strong = scores_on_inds.clone().detach()\n",
    "        scores_on_inds_strong[:, torch.tensor(key_inds)] += 100\n",
    "        prob_on_inds_strong = F.softmax(scores_on_inds_strong / temperature, dim=-1)\n",
    "        gt_prob_strong = prob_on_inds_strong[torch.arange(len(prob_on_inds_strong)), gt_on_inds]\n",
    "        tuple_prob_strong = gt_prob_strong[::2] * gt_prob_strong[1::2]\n",
    "\n",
    "        tuple_probs_raw[level].extend(tuple_prob_raw.tolist())\n",
    "        tuple_probs_weak[level].extend(tuple_prob_weak.tolist())\n",
    "        tuple_probs_strong[level].extend(tuple_prob_strong.tolist())\n",
    "\n",
    "raw_means = np.mean(tuple_probs_raw, axis=1)\n",
    "weak_means = np.mean(tuple_probs_weak, axis=1)\n",
    "strong_means = np.mean(tuple_probs_strong, axis=1)\n",
    "\n",
    "sampling_raw_1_4 = np.mean(raw_means[:4]) * 100\n",
    "sampling_raw_5 = np.mean(raw_means[4]) * 100\n",
    "\n",
    "sampling_weak_1_4 = np.mean(weak_means[:4]) * 100\n",
    "sampling_weak_5 = np.mean(weak_means[4]) * 100\n",
    "\n",
    "sampling_strong_1_4 = np.mean(strong_means[:4]) * 100\n",
    "sampling_strong_5 = np.mean(strong_means[4]) * 100\n",
    "\n",
    "\n",
    "temperature = 1e-5\n",
    "\n",
    "n_levels = 5\n",
    "\n",
    "tuple_probs_raw = [[] for _ in range(n_levels)]\n",
    "tuple_probs_weak = [[] for _ in range(n_levels)]\n",
    "tuple_probs_strong = [[] for _ in range(n_levels)]\n",
    "\n",
    "for seq in input_seqs:\n",
    "    input_ids = tokenizer.encode(seq, return_tensors='pt').to(model.device)\n",
    "    sep_inds = get_output_inds(input_ids[:, :-1], tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "    gt_outputs = input_ids[:, 1:]\n",
    "\n",
    "    for level, inds in enumerate(sep_inds):\n",
    "\n",
    "        gt_on_inds = gt_outputs[0][torch.tensor(inds).to(model.device)]\n",
    "\n",
    "        scores_on_inds = outputs.logits[0][torch.tensor(inds).to(model.device)]\n",
    "        prob_on_inds_raw = F.softmax(scores_on_inds / temperature, dim=-1)\n",
    "        gt_prob_raw = prob_on_inds_raw[torch.arange(len(prob_on_inds_raw)), gt_on_inds]\n",
    "        tuple_prob_raw = gt_prob_raw[::2] * gt_prob_raw[1::2]\n",
    "\n",
    "        scores_on_inds_weak = scores_on_inds.clone().detach()\n",
    "        scores_on_inds_weak[:, 128108] = -10000\n",
    "        prob_on_inds_weak = F.softmax(scores_on_inds_weak / temperature, dim=-1)\n",
    "        gt_prob_weak = prob_on_inds_weak[torch.arange(len(prob_on_inds_weak)), gt_on_inds]\n",
    "        tuple_prob_weak = gt_prob_weak[::2] * gt_prob_weak[1::2]\n",
    "\n",
    "        key_inds = torch.tensor(letter_token_inds[level + 1]).to(model.device)\n",
    "        scores_on_inds_strong = scores_on_inds.clone().detach()\n",
    "        scores_on_inds_strong[:, torch.tensor(key_inds)] += 100\n",
    "        prob_on_inds_strong = F.softmax(scores_on_inds_strong / temperature, dim=-1)\n",
    "        # print(prob_on_inds_strong.sum(dim=-1))\n",
    "        gt_prob_strong = prob_on_inds_strong[torch.arange(len(prob_on_inds_strong)), gt_on_inds]\n",
    "        tuple_prob_strong = gt_prob_strong[::2] * gt_prob_strong[1::2]\n",
    "\n",
    "        # print(tuple_prob_strong - tuple_prob_weak)\n",
    "\n",
    "        tuple_probs_raw[level].extend(tuple_prob_raw.tolist())\n",
    "        tuple_probs_weak[level].extend(tuple_prob_weak.tolist())\n",
    "        tuple_probs_strong[level].extend(tuple_prob_strong.tolist())\n",
    "\n",
    "            \n",
    "raw_means = np.mean(tuple_probs_raw, axis=1)\n",
    "weak_means = np.mean(tuple_probs_weak, axis=1)\n",
    "strong_means = np.mean(tuple_probs_strong, axis=1)\n",
    "\n",
    "greedy_raw_1_4 = np.mean(raw_means[:4]) * 100\n",
    "greedy_raw_5 = np.mean(raw_means[4]) * 100\n",
    "\n",
    "greedy_weak_1_4 = np.mean(weak_means[:4]) * 100\n",
    "greedy_weak_5 = np.mean(weak_means[4]) * 100\n",
    "\n",
    "greedy_strong_1_4 = np.mean(strong_means[:4]) * 100\n",
    "greedy_strong_5 = np.mean(strong_means[4]) * 100\n",
    "\n",
    "print(\"None:\\t ${:.2f}\\%$ & ${:.2f}\\%$ & ${:.2f}\\%$ & ${:.2f}\\%$\".format(greedy_raw_1_4, greedy_raw_5, sampling_raw_1_4, sampling_raw_5))\n",
    "print(\"Weak:\\t ${:.2f}\\%$ & ${:.2f}\\%$ & ${:.2f}\\%$ & ${:.2f}\\%$\".format(greedy_weak_1_4, greedy_weak_5, sampling_weak_1_4, sampling_weak_5))\n",
    "print(\"Strong:\\t ${:.2f}\\%$ & ${:.2f}\\%$ & ${:.2f}\\%$ & ${:.2f}\\%$\".format(greedy_strong_1_4, greedy_strong_5, sampling_strong_1_4, sampling_strong_5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
